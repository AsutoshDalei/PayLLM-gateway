{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c915c8b-8e32-4870-bdaf-aa6118a70f96",
   "metadata": {},
   "source": [
    "# Testing out Local LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d25a7e3-d464-477f-9c07-417f6fd62be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a12c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ebef53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are PayLLM, a conversational payment assistant. NEVER call tools until you have collected all required information:  You should never use external knowledge, assumptions or information beyond what is explicitly shared or recieved. Follow this structured flow and do NOT call tools unless all required information is available: 1. Greet the user if they greet you. 2. Ask for the user their state and the service they want to pay the bill for. 3. Get the list of service providers using fetch_service_provider tool. 4. Ask for the service provider. 5. Ask for the bill number. 6. Confirm fetching the bill. 7. Display the bill amount. 8. Ask for payment confirmation. 9. Confirm payment. Maintain the flow based on previous responses.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = (\"You are PayLLM, a conversational payment assistant. NEVER call tools until you have collected all required information:\\n\\n\"\n",
    "\"You should never use external knowledge, assumptions or information beyond what is explicitly shared or recieved.\\n\"\n",
    "\"Follow this structured flow and do NOT call tools unless all required information is available:\\n\"\n",
    "\"1. Greet the user if they greet you.\\n\"\n",
    "\"2. Ask for the user their state and the service they want to pay the bill for.\\n\"\n",
    "\"3. Get the list of service providers using fetch_service_provider tool.\\n\"\n",
    "\"4. Ask for the service provider.\\n\"\n",
    "\"5. Ask for the bill number.\\n\"\n",
    "\"6. Confirm fetching the bill.\\n\"\n",
    "\"7. Display the bill amount.\\n\"\n",
    "\"8. Ask for payment confirmation.\\n\"\n",
    "\"9. Confirm payment.\\n\"\n",
    "\"Maintain the flow based on previous responses.\")\n",
    "\n",
    "' '.join(tt.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "844092bb-1bd2-4037-baaf-5ed9f413838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "# llm = ChatOllama(model=\"llama3.2:1b\", temperature=0)\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d955cf4-be35-4075-b37d-3ea3c2772731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To proceed with paying your bill, I need to know a few details first.\\n\\nCan you please tell me the number of the bill you'd like to pay?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-02-12T22:14:36.447587Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1133021959, 'load_duration': 29848625, 'prompt_eval_count': 127, 'prompt_eval_duration': 201000000, 'eval_count': 33, 'eval_duration': 900000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-d4d26844-08a5-4aad-9a64-013bf559dd9c-0', usage_metadata={'input_tokens': 127, 'output_tokens': 33, 'total_tokens': 160})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialSystemMessage = '''You are an excellent virtual assistant to help with payment of bills.\n",
    "You should follow the following steps strictly to pay the bills:\n",
    "1. Ask the user the bill number\n",
    "2. Ask the user if you should fetch the bill details.\n",
    "3. Inform the user the bill details once fetched.\n",
    "4. Ask the user if you should pay the bill.\n",
    "5. Is the user agrees, pay the bill.\n",
    "\n",
    "You should never user external knowledge, assumptions or information beyond what is explicitly shared or recieved.\n",
    "'''\n",
    "\n",
    "\n",
    "messages = [\n",
    "    (\"system\",initialSystemMessage),\n",
    "    (\"human\", \"Pay my bill\")\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce8425-ee02-4c78-afb8-e0a206f795fe",
   "metadata": {},
   "source": [
    "## Testing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae52d71-5e10-4665-9499-9f2f21833d05",
   "metadata": {},
   "source": [
    "### Calculator Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1556dfef-a5f2-4ef0-afde-693a75ff0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add (+) two numbers\"\"\"\n",
    "    return a+b\n",
    "    \n",
    "@tool\n",
    "def sub(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract (-) two numbers\"\"\"\n",
    "    return a-b\n",
    "    \n",
    "@tool\n",
    "def mul(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply (*, x) two numbers\"\"\"\n",
    "    return a*b\n",
    "    \n",
    "@tool\n",
    "def div(a: int, b: int) -> int:\n",
    "    \"\"\"Divide (/) two numbers\"\"\"\n",
    "    return a/b\n",
    "\n",
    "@tool\n",
    "def fetchUserDets(name: str) -> str:\n",
    "    \"\"\"Function to fetch user details. Input is name of the user. Output relevant details.\"\"\"\n",
    "    # hmap = \n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "tools = [mul, div, add, sub]\n",
    "toolsMap = {'add':add, \"sub\":sub, \"mul\":mul, \"div\":div}\n",
    "\n",
    "llmTools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0cdbb0-94b7-4241-a71f-996319510ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77b6c9a-6435-468e-b793-1e7b0f703e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-02-24T23:26:50.365703Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1046341750, 'load_duration': 33897542, 'prompt_eval_count': 347, 'prompt_eval_duration': 215000000, 'eval_count': 28, 'eval_duration': 795000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-5029179c-cd05-42d2-842b-2a7f364e646f-0', tool_calls=[{'name': 'add', 'args': {'a': '3 * 12', 'b': '8 * 213'}, 'id': '2d90d4f5-9bda-497f-b61b-4514d504997c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 347, 'output_tokens': 28, 'total_tokens': 375})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Add 3 and 4?\"\n",
    "query = \"What is 3 multiplied 12? Also, what is 11 + 49?\"\n",
    "\n",
    "# llmTools.invoke(query).tool_calls\n",
    "\n",
    "messages = [\n",
    "    (\"system\",\"You are a helpful assistant. be kind\"),\n",
    "    (\"human\", \"Tell me the answer to 3 * 12+8 * 213\")\n",
    "]\n",
    "ai_msg = llmTools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2d74fa-2cd2-480f-8b64-107c5b25dcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'a': '3 * 12', 'b': '8 * 213'},\n",
       "  'id': '2d90d4f5-9bda-497f-b61b-4514d504997c',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75446aba-e14a-4498-88fe-100b5ed65215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('system', 'You are a helpful assistant. be kind'),\n",
       " ('human', 'Tell me the answer to 3 * 12+8 * 213'),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-02-24T23:26:50.365703Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1046341750, 'load_duration': 33897542, 'prompt_eval_count': 347, 'prompt_eval_duration': 215000000, 'eval_count': 28, 'eval_duration': 795000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-5029179c-cd05-42d2-842b-2a7f364e646f-0', tool_calls=[{'name': 'add', 'args': {'a': '3 * 12', 'b': '8 * 213'}, 'id': '2d90d4f5-9bda-497f-b61b-4514d504997c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 347, 'output_tokens': 28, 'total_tokens': 375})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e11b63-9d16-4ad8-9c9c-17f4e1893d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0933bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sub',\n",
       " 'args': {'a': 11, 'b': 49},\n",
       " 'id': '2067c8e1-3fa8-435b-beba-7b95f6cfca06',\n",
       " 'type': 'tool_call'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17aa6318-8cc8-4256-9683-81acf6dc2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = llmTools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5513aaf5-cfbf-4ee0-b14a-a9d21318949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to 3 * 12 is 36.\n",
      "\n",
      "The answer to 11 - 49 is -38.\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc85ddc1-4f89-4a8d-b8d4-a35f39677769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The answer to 3 * 12 is 36.\\n\\nThe answer to 11 - 49 is -38.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-02-24T23:24:26.259054Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1108464084, 'load_duration': 31862125, 'prompt_eval_count': 142, 'prompt_eval_duration': 388000000, 'eval_count': 25, 'eval_duration': 685000000, 'message': Message(role='assistant', content='The answer to 3 * 12 is 36.\\n\\nThe answer to 11 - 49 is -38.', images=None, tool_calls=None)}, id='run-a1d082b4-c1cd-4821-b174-4d7676facfed-0', usage_metadata={'input_tokens': 142, 'output_tokens': 25, 'total_tokens': 167})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd6eab1-dcee-4fbf-996f-79c503343156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to 3 * 12 is 36.\n",
      "\n",
      "The answer to 11 - 49 is -38.\n"
     ]
    }
   ],
   "source": [
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802134be-f20a-4a53-96e7-9799f1b4e651",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647913ff-0048-4abe-a09c-62b4324f980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "import traceback\n",
    "\n",
    "# Initialize the LLaMA 3.2 model\n",
    "model = ChatOllama(model=\"llama3.2:latest\", temperature=0)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c690731",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Answer the question below using the context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt_str = \"Tell me an short fact about {topic}\"\n",
    "\n",
    "prompt_str = \"\"\"\n",
    "You are a helpful assistant.\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/py587mbx6nq4z2vh3d9b_5780000gn/T/ipykernel_3979/2591332229.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "prompt\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# chain = prompt | model | output_parser\n",
    "\n",
    "chain = ({\"human_input\": lambda x: x, \"chat_history\": memory.load_memory_variables}| prompt | llm | memory.save_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d0292e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a short fact:\\n\\nThe Boeing 747 jumbo jet has a unique feature called \"hump\" on its upper deck, which is actually a raised section that provides additional storage space and allows passengers to walk around the cabin more easily.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'topic':'aeroplane'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a6fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
